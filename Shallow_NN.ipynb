{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Nerual Network With Single Hidden Layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**README**: This derivation and go-through notes is made based on my own mathematical proof and may contain errors here or there. If you find one, don't hesitate to reach me out and point out where I have made mistakes.\n",
    "\n",
    "Email: yd1@andrew.cmu.edu\n",
    "\n",
    "This notebook will go through the math derivation of forward and backpropogation of the nerual network with one hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/nn_1.png\" style=\"width:600px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background**:\n",
    "In this figure, we have a single training example that is connected with 3 neuron (hidden layer) and combined together to output the output layer\n",
    "\n",
    "**Clarification**:\n",
    "The following are some of the notations or functions that will be used in the derivation (continously adding)\n",
    "    - Activation function for hidden layer is tanh, and output layer is sigmoid (for the reason, please refer to Prof.Andrew Ng's lecture notes)\n",
    "    - Capital letters refer to the matrices that spans over the entire training set.\n",
    "    - m : number of training examples (size)\n",
    "    - n_x : feature dimension of a single training example\n",
    "    - n_h : size of hidden layer\n",
    "    - n_y : size of output layer (for binary classfication n_y = 2)\n",
    "    - i : i th training example\n",
    "    - square bracket : the nth layer (i.e. [1]} means the hidden layer)\n",
    "    - loss function - cross entropy (for classification)\n",
    "    - cost function (mean loss):\n",
    "<br>\n",
    "$$J = - \\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}log(a^{[2](i)}) + (1 - y^{(i)})log(1 - a^{[2](i)}))$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the whole model is really well represented by the following picture (by Prof.Andrew Ng), even though the dimension may not consistent with this note.\n",
    "<img src=\"images/nn_2.png\" style=\"width:600px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propogation\n",
    "<br>\n",
    "$$Z^{[1]} = W^{[1]} X + b^{[1]}$$\n",
    "<br>\n",
    "$$A^{[1]} = g{(Z^{[1]})}$$\n",
    "<br>\n",
    "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$$\n",
    "<br>\n",
    "$$A^{[2]} = g{(Z^{[2]})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of bugs in the code are caused by incorrect dimension of the matrix, so it is important to check the dimension throughout the calculation.\n",
    "\n",
    "\n",
    "**Sanity Check: Dimensionality**\n",
    "\n",
    "- First layer (hidden layer)\n",
    "$$X : (n_x, m)$$\n",
    "$$W^{[1]} : (n_h, n_x)$$\n",
    "$$W^{[1]} \\cdot X : (n_h, m)$$\n",
    "$$b^{[1]} : (n_h, m)$$\n",
    "$$Z^{[1]}, A^{[1]} : (n_h, m)$$\n",
    "\n",
    "- output layer\n",
    "$$W^{[2]} : (n_y, n_h)$$\n",
    "$$W^{[1]} \\cdot X : (n_y, m)$$\n",
    "$$b^{[2]} : (n_y, m)$$\n",
    "$$Z^{[2]}, A^{[2]} : (n_y, m)$$\n",
    "\n",
    "now, we have m training examples and each of them output consists of the $\\textbf{probability}$ of two binary classfication results, and combined with\n",
    "\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & if a^{[2](i)} > 0.5 \\\\ 0 & otherwise\\end{cases}\\tag{5}$$\n",
    "\n",
    "can produce a (1, m) matrix with predicted $\\hat y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed() \n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation function\n",
    "def sigmoid(s):\n",
    "    return 1 / (1 + np.exp(-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    # Use tanh for the activation function in the hidden layer\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(you can use either `np.multiply()` and then `np.sum()` or directly `np.dot()`).  \n",
    "Note that if you use `np.multiply` followed by `np.sum` the end result will be a type `float`, whereas if you use `np.dot`, the result will be a 2D numpy array.  We can use `np.squeeze()` to remove redundant dimensions (in the case of single float, this will be reduced to a zero-dimension array). We can cast the array as a type `float` using `float()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the cost of training \n",
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (1)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (1)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))\n",
    "    cost = - 1/m * (np.sum(logprobs, axis=1, keepdims=True)) \n",
    "    \n",
    "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
    "                                    # E.g., turns [[17]] into 17 \n",
    "        \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propogation -- Gradient Descent **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clarification** \n",
    "The following derivation will denote derivative of $\\mathcal{L}$ respect to any variable as $\\partial {var}$ i.e. \n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial a} := \\partial a$\n",
    "$$\\mathcal{L}(A, Y) = -Y\\log(A) - (1 - A) \\log(1 - A)$$ \n",
    "<br>\n",
    "$$\\partial A^{[2]} = \\frac{\\partial \\mathcal{L}}{\\partial A} = -\\frac{y}{A} +\\frac{1-Y}{1-A}$$\n",
    "\n",
    "then, use chain rule to calculat the $\\partial Z^{[1]}$, capital letters represent matrix over all training examples\n",
    "<b>Note</b> here, for output layer our activation function is still sigmoid.\n",
    "$$\\partial Z^{[2]} = \\frac{\\partial \\mathcal{L}}{\\partial A^{[2]}}*\\frac{\\partial A^{[2]}}{\\partial Z^{[2]}}$$\n",
    "<br>\n",
    "$$ =  (-\\frac{Y}{A^{[2]}} +\\frac{1 - Y}{1 - A^{[2]}})*(\\sigma(Z^{[2]})(1 - \\sigma(Z^{[2]})) )$$\n",
    "<br>\n",
    "$$ = (-\\frac{Y}{\\sigma(Z^{[2]})} +\\frac{1 - Y}{1-\\sigma(Z^{[2]})})*(\\sigma(Z^{[2]})(1 - \\sigma(Z^{[2]})))$$\n",
    "<br>\n",
    "$$ = -Y(1 - \\sigma(Z^{[2]})) + \\sigma(Z^{[2]})(1 - Y)$$\n",
    "<br>\n",
    "$$ = \\sigma(Z^{[2]}) - Y = A^{[2]} - Y $$\n",
    "\n",
    "now, its time to compute $\\partial W^{[2]}$ and $\\partial b^{[2]}$\n",
    "$$ \\partial W^{[2]} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{[2]}} \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} = \\partial Z^{[2]} \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} = \\partial Z^{[2]} A^{[1] T}$$\n",
    "\n",
    "<b>Note</b>, the transpose of $A^{[1]}$ is because the $\\partial Z^{[2]}$ is a $(n_y, m)$ matrix and $A^{[1]}$ is $(n_h, m)$ matrix. In order to perform matrix multiplication, need to transpose the $A^{[2]}$, and now $\\partial W^{[2]}$ is a $(n_y, n_h)$ matrix. Coherent withprevious $W^{[2]}$'s dimension.\n",
    "\n",
    "Similarly:\n",
    "$$ \\partial b^{[2]} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{[2]}} \\frac{\\partial Z^{[2]}}{\\partial b^{[2]}} = \\partial Z^{[2]} \\frac{\\partial Z^{[2]}}{\\partial b^{[2]}} = \\partial Z^{[2]}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's compute the $\\partial Z^{[1]}$ (p.s. I think this is the most difficult part in the derivation)\n",
    "\n",
    "$$ \\partial Z^{[1]} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{[2]}}\\frac{\\partial Z^{[2]}}{\\partial A^{[1]}}\\frac{\\partial A^{[1]}}{\\partial Z^{[1]}}$$\n",
    "\n",
    " Since in the middle hidden layer the activation we implemented will be tanh, thus let $ g(x) = tanh(x)$, therefore, $A^{[1]} = g(Z^{[1]}) = tanh(Z^{[1]})$, and corresponding derivitive is $g^{'} = 1 - Z^{[1]2}$ and '*' denotes the element-wise multiplication, in numpy its `np.multiply`\n",
    "\n",
    "$$ = W^{[2]T} \\partial Z^{[2]} * (g'(Z^{[1]}))$$\n",
    "<br>\n",
    "Up to this point, it is again very necessary to do a sanity check of dimension to make sure the matrices have right dimensions.<br>\n",
    "$$ Z^{[1]}, dZ^{[1]} : (n_h, m)$$\n",
    "<br>\n",
    "$$ Z^{[2]}, dZ^{[2]} : (n_y, m)$$\n",
    "<br>\n",
    "$$dW^{[2]} : (n_y, n_h)$$\n",
    "<br>\n",
    "$$ dim (W^{[2]T}\\partial Z^{[2]} * (g'(Z^{[1]})) = (n_y, n_h)^{T}(n_y, m) * (n_h, m) $$\n",
    "<br>\n",
    "$$ = (n_h, m) = dim(Z^{[1]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, compute $\\partial W^{[1]}$ and $\\partial b^{[1]}$\n",
    "$$ \\partial W^{[1]} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{[1]}} \\frac{\\partial Z^{[1]}}{\\partial W^{[1]}} = \\partial Z^{[1]} X^{[T]}$$\n",
    "<br>\n",
    "$$ \\partial b^{[1]} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{[1]}} \\frac{\\partial Z^{[1]}}{\\partial b^{[1]}} = \\partial Z^{[1]} \\frac{\\partial Z^{[1]}}{\\partial b^{[1]}} = \\partial Z^{[1]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis = 1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), (1 - np.power(A1,2))) \n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis = 1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the parameters\n",
    "\n",
    "**General gradient descent rule**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration of the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "                \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction \n",
    "\n",
    "**Reminder**:\n",
    "predictions = $y_{prediction}$ = $\n",
    "\\begin{cases}\n",
    "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "\\end{cases}$  \n",
    "    \n",
    "As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: ```X_new = (X > threshold)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2 > 0.5)    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "- http://scs.ryerson.ca/~aharley/neural-networks/\n",
    "- http://cs231n.github.io/neural-networks-case-study/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
